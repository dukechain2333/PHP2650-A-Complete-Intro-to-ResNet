{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbWIl12L9uuMWmZ7D0gTIb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_gOoukBgZUGt"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.models import resnet50\n","from torch import nn, optim"]},{"cell_type":"markdown","source":["# Training ResNet-50 on CIFAR-10"],"metadata":{"id":"1tqYD9iZZgW_"}},{"cell_type":"code","source":["class RandomResize(transforms.Resize):\n","    def __init__(self, min_size=256, max_size=480, interpolation=transforms.InterpolationMode.BILINEAR):\n","        super().__init__(size=1)\n","        self.min_size = min_size\n","        self.max_size = max_size\n","        self.interpolation = interpolation\n","\n","    def forward(self, img):\n","        size = torch.randint(self.min_size, self.max_size + 1, (1,)).item()\n","        short, long = min(img.size[1], img.size[0]), max(img.size[1], img.size[0])\n","        if short == img.size[0]:\n","            ow, oh = size, int(size * long / short)\n","        else:\n","            ow, oh = int(size * long / short), size\n","        return F.resize(img, (oh, ow), self.interpolation)\n","\n","mean = [0.4914, 0.4822, 0.4465]\n","std = [1.0, 1.0, 1.0]\n","\n","transform = transforms.Compose([\n","    RandomResize(),\n","    transforms.RandomCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=mean, std=std)\n","])\n","\n","train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)\n","\n","test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False)"],"metadata":{"id":"ac93mP_XZaoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = resnet50(pretrained=False)\n","model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","model.maxpool = nn.Identity()\n","model.fc = nn.Linear(model.fc.in_features, 10)"],"metadata":{"id":"fSemduoxZcnJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","train_losses = []\n","val_losses = []\n","\n","for epoch in range(100):\n","    model.train()\n","    train_loss = 0.0\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    avg_train_loss = train_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(test_loader)\n","    val_losses.append(avg_val_loss)\n","\n","    print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')"],"metadata":{"id":"LtQd2-tvZeP-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Use Pre-trained ResNet-50"],"metadata":{"id":"W3KKn5PfZkz3"}},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)"],"metadata":{"id":"EC_zcUJcZmGy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = resnet50(pretrained=True)\n","model.fc = nn.Linear(model.fc.in_features, 10)\n","model.eval()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","model.train()\n","for epoch in range(50):\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if i % 2000 == 1999:\n","            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / 2000:.3f}')\n","            running_loss = 0.0"],"metadata":{"id":"4OIS0xtgZ4Az"},"execution_count":null,"outputs":[]}]}