{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPi5xpdmw52J4iY5bYOOxMb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"B6TGt3Ry8e6e","executionInfo":{"status":"ok","timestamp":1714933725155,"user_tz":240,"elapsed":5872,"user":{"displayName":"William Qian","userId":"05390114614281547230"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision import datasets, transforms, utils\n","import torchvision.transforms as transforms\n","from torchvision.transforms import functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","source":["class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n"],"metadata":{"id":"HFfJ_VaV8jmo","executionInfo":{"status":"ok","timestamp":1714933725155,"user_tz":240,"elapsed":2,"user":{"displayName":"William Qian","userId":"05390114614281547230"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class ResNet(nn.Module):\n","    def __init__(self, block, layers, num_classes):\n","        super(ResNet, self).__init__()\n","        self.in_channels = 64\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","    def _make_layer(self, block, out_channels, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.in_channels != out_channels * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_channels * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.in_channels, out_channels, stride, downsample))\n","        self.in_channels = out_channels * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.in_channels, out_channels))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","        return x\n"],"metadata":{"id":"8cTeNf678nU-","executionInfo":{"status":"ok","timestamp":1714933725155,"user_tz":240,"elapsed":1,"user":{"displayName":"William Qian","userId":"05390114614281547230"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def resnet50():\n","    return ResNet(Bottleneck, [3, 4, 6, 3], 10)"],"metadata":{"id":"lmSKwRml8piS","executionInfo":{"status":"ok","timestamp":1714933725155,"user_tz":240,"elapsed":1,"user":{"displayName":"William Qian","userId":"05390114614281547230"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["model = resnet50()"],"metadata":{"id":"sys4fgCN8sqq","executionInfo":{"status":"ok","timestamp":1714933725767,"user_tz":240,"elapsed":613,"user":{"displayName":"William Qian","userId":"05390114614281547230"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class RandomResize(transforms.Resize):\n","    def __init__(self, min_size=256, max_size=480, interpolation=transforms.InterpolationMode.BILINEAR):\n","        super().__init__(size=1)\n","        self.min_size = min_size\n","        self.max_size = max_size\n","        self.interpolation = interpolation\n","\n","    def forward(self, img):\n","        size = torch.randint(self.min_size, self.max_size + 1, (1,)).item()\n","        short, long = min(img.size[1], img.size[0]), max(img.size[1], img.size[0])\n","        if short == img.size[0]:\n","            ow, oh = size, int(size * long / short)\n","        else:\n","            ow, oh = int(size * long / short), size\n","        return F.resize(img, (oh, ow), self.interpolation)\n","\n","mean = [0.4914, 0.4822, 0.4465]\n","std = [1.0, 1.0, 1.0]\n","\n","transform = transforms.Compose([\n","    RandomResize(),\n","    transforms.RandomCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=mean, std=std)\n","])\n","\n","train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)\n","\n","test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l5h1gV2x8y4r","executionInfo":{"status":"ok","timestamp":1714933730690,"user_tz":240,"elapsed":4925,"user":{"displayName":"William Qian","userId":"05390114614281547230"}},"outputId":"211b9218-5da2-49b1-a4e6-c33c5203f856"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:01<00:00, 104052372.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","train_losses = []\n","val_losses = []\n","\n","for epoch in range(100):\n","    model.train()\n","    train_loss = 0.0\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    avg_train_loss = train_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(test_loader)\n","    val_losses.append(avg_val_loss)\n","\n","    print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6FCHU2zU9YwO","outputId":"e3b86aa5-1672-46bd-f726-1f0a7f7c6e03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Training Loss: 2.0875725004137777, Validation Loss: 1.9539063632488252\n","Epoch 2, Training Loss: 1.8399242795243556, Validation Loss: 1.7396225333213806\n","Epoch 3, Training Loss: 1.6580842775957925, Validation Loss: 1.6346054196357727\n","Epoch 4, Training Loss: 1.4832148180932414, Validation Loss: 1.6019564807415008\n","Epoch 5, Training Loss: 1.3616846112572416, Validation Loss: 1.4172810971736909\n","Epoch 6, Training Loss: 1.2745198802072175, Validation Loss: 1.459951189160347\n","Epoch 7, Training Loss: 1.1992890615852512, Validation Loss: 1.3394732028245926\n","Epoch 8, Training Loss: 1.1334146267297316, Validation Loss: 1.1774198561906815\n","Epoch 9, Training Loss: 1.08233678006396, Validation Loss: 1.1110988184809685\n","Epoch 10, Training Loss: 1.0230083413878266, Validation Loss: 1.1513228431344031\n","Epoch 11, Training Loss: 0.9804264492526347, Validation Loss: 1.0033413127064705\n","Epoch 12, Training Loss: 0.9500029163092983, Validation Loss: 0.9998329848051071\n","Epoch 13, Training Loss: 0.9092063240858973, Validation Loss: 1.0517763301730156\n","Epoch 14, Training Loss: 0.8894119797920694, Validation Loss: 0.910308127105236\n","Epoch 15, Training Loss: 0.8468345732105022, Validation Loss: 1.0093781545758247\n","Epoch 16, Training Loss: 0.829459724681718, Validation Loss: 0.8908561959862709\n","Epoch 17, Training Loss: 0.8012005188027207, Validation Loss: 0.9499599739909173\n","Epoch 18, Training Loss: 0.7802865991787035, Validation Loss: 0.8374197021126747\n","Epoch 19, Training Loss: 0.769467261068675, Validation Loss: 0.9004943504929542\n","Epoch 20, Training Loss: 0.7522674339766405, Validation Loss: 0.7891527459025383\n","Epoch 21, Training Loss: 0.7292435166178918, Validation Loss: 0.8597001418471336\n","Epoch 22, Training Loss: 0.705367770122022, Validation Loss: 0.7592654764652252\n","Epoch 23, Training Loss: 0.6976116208397612, Validation Loss: 0.7730315223336219\n","Epoch 24, Training Loss: 0.6764988963093076, Validation Loss: 0.7491370588541031\n","Epoch 25, Training Loss: 0.6643838762324683, Validation Loss: 0.6985670797526836\n","Epoch 26, Training Loss: 0.6432497513537504, Validation Loss: 0.6926422163844108\n","Epoch 27, Training Loss: 0.6327505219651728, Validation Loss: 0.7054788284003735\n","Epoch 28, Training Loss: 0.6279334111177192, Validation Loss: 0.6897621512413025\n","Epoch 29, Training Loss: 0.6080575237164692, Validation Loss: 0.6865253627300263\n","Epoch 30, Training Loss: 0.5930869710080477, Validation Loss: 0.6699853003025055\n","Epoch 31, Training Loss: 0.5819276791750169, Validation Loss: 0.7279910638928413\n","Epoch 32, Training Loss: 0.5746217693905441, Validation Loss: 0.6581981644034386\n","Epoch 33, Training Loss: 0.5769660148997696, Validation Loss: 0.6437671720981598\n","Epoch 34, Training Loss: 0.5576341153711689, Validation Loss: 0.6395116567611694\n","Epoch 35, Training Loss: 0.5461074171625838, Validation Loss: 0.6831909820437432\n","Epoch 36, Training Loss: 0.5373576073622217, Validation Loss: 0.6673716992139817\n","Epoch 37, Training Loss: 0.5319011819605924, Validation Loss: 0.6188403911888599\n","Epoch 38, Training Loss: 0.5126917769714278, Validation Loss: 0.646451073884964\n","Epoch 39, Training Loss: 0.5093666681525658, Validation Loss: 0.6149444565176964\n","Epoch 40, Training Loss: 0.5004705873375036, Validation Loss: 0.6158882580697537\n","Epoch 41, Training Loss: 0.493660252161172, Validation Loss: 0.6100939206779004\n","Epoch 42, Training Loss: 0.4831663763948849, Validation Loss: 0.5861138887703419\n"]}]},{"cell_type":"code","source":["plt.figure(figsize=(10, 5))\n","plt.plot(train_losses, label='Training Loss')\n","plt.plot(val_losses, label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"id":"tRCku5PK_1n4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy: {100 * correct / total}%')\n"],"metadata":{"id":"AtK8e6529e2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images, labels = next(iter(test_loader))\n","images = images[:4]\n","labels = labels[:4]\n","\n","images = images.to(device)\n","labels = labels.to(device)\n","\n","model.eval()\n","with torch.no_grad():\n","    outputs = model(images)\n","\n","predicted_labels = torch.argmax(outputs, dim=1)\n","\n","fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n","\n","for i in range(len(images)):\n","    axs[i // 2, i % 2].imshow(images[i].cpu().numpy().transpose((1, 2, 0)))\n","    axs[i // 2, i % 2].set_title(f\"Predicted: {predicted_labels[i].item()}, Actual: {labels[i].item()}\")\n","\n","plt.show()\n"],"metadata":{"id":"FICRz78FKfY4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model, 'self_trained_resnet50_cifar10.pth')"],"metadata":{"id":"UHeG0rwLMHFo"},"execution_count":null,"outputs":[]}]}